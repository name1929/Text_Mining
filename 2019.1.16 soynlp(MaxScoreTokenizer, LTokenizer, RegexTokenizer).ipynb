{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer, LTokenizer, RegexTokenizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MaxScoreTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "띄어쓰기가 잘 안 된 한국어 문장을 토크나이징 해주는 unsupervised tokenizer이다. 점수를 잘 정의하면 단어 추출의 질이 상승한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-1] Score 값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'파스':0.3, '파스타':0.7, '좋아요' :0.2, '좋아':0.5}\n",
    "#key : string형태, value: float 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-2] MaxScoreTokenizer설정 및 문장 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['파스타', '가', '좋아', '요']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MaxScoreTokenizer(scores = scores)\n",
    "tokenizer.tokenize('파스타가좋아요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-3] 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxTokenizer 출력 : (subword, begin, end, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten=False\n",
      "[[('난', 0, 1, 0.0, 1), ('파스타', 1, 4, 0.7, 3), ('가', 4, 5, 0.0, 1)],\n",
      " [('좋아', 0, 2, 0.5, 2), ('요', 2, 3, 0.0, 1)]]\n",
      "\n",
      "flatten=True\n",
      "['난', '파스타', '가', '좋아', '요']\n"
     ]
    }
   ],
   "source": [
    "print('flatten=False')\n",
    "pprint(tokenizer.tokenize('난파스타가 좋아요', flatten =False))\n",
    "\n",
    "print('\\nflatten=True')\n",
    "pprint(tokenizer.tokenize('난파스타가 좋아요', flatten= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1-3] 한계점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    운동에 관한 기사라고 가정하면 '파스'에 좀 더 높은 점수를 줄 것이다. \n",
    "    그러나, 파스와, 파스타 둘 다 뜻하는 문장이 있다면 다시 고민해봐야 한다.\n",
    "    문서의 길이가 늘어날 수록 score 선정이 복잡해 질 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "띄어쓰기가 잘 돼있는 한국어 문서에는 MaxScoreTokenizer가 필요없다. 한국어는 L+[R] 구조이기 때문이다. 이때는 한 어절의 왼쪽에서 부터 글자 점수가 가장 높은 부분을 기준으로 토크나이징한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2-1] Score 값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.tokenizer import LTokenizer \n",
    "\n",
    "scores = {'데이':0.5, '데이터':0.5, '데이터마이닝':0.5, '공부':0.5, '공부중':0.45}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2-2] LTokenizer 설정 및 문장 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Cohesion score 를 이용하여 어절의 왼쪽에 위치한 substrings 중 가장 단어스러운 부분을 찾는 방식으로 간단한 비지도학습 기반 토크나이저를 만들 수 있습니다.\n",
    "\n",
    "    뉴스처럼 띄어쓰기가 잘 되어 있는 한국어 텍스트의 어절 구조는 명확히 L + [R] 입니다. 어절의 왼쪽에 위치한 substring 중 단어 점수가 가장 높은 부분을 선택하면 어절을 L + [R] 로 나눌 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LTokenizer(scores= scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "flatten=True\n",
      "sent = 데이터마이닝을 공부한다\n",
      "['데이터마이닝', '을', '공부', '한다']\n",
      "\n",
      "flatten=False\n",
      "sent =데이터마이닝을 공부한다\n",
      "[('데이터마이닝', '을'), ('공부', '한다')]\n"
     ]
    }
   ],
   "source": [
    "print('\\nflatten=True\\nsent = 데이터마이닝을 공부한다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부한다'))\n",
    "\n",
    "print('\\nflatten=False\\nsent =데이터마이닝을 공부한다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부한다', flatten=False))\n",
    "\n",
    "scores = {'데이':0.5, '데이터':0.5, '데이터마이닝':0.5, '공부':0.5, '공부중':0.45}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "flatten=True\n",
      "sent =데이터분석을 위해서 데이터마이닝을 공부한다\n",
      "['데이터', '분석을', '위해서', '데이터마이닝', '을', '공부', '한다']\n",
      "\n",
      "flatten=False\n",
      "sent =데이터분석을 위해서 데이터마이닝을 공부한다\n",
      "[('데이터', '분석을'), ('위해서', ''), ('데이터마이닝', '을'), ('공부', '한다')]\n"
     ]
    }
   ],
   "source": [
    "print('\\nflatten=True\\nsent =데이터분석을 위해서 데이터마이닝을 공부한다')\n",
    "print(tokenizer.tokenize('데이터분석을 위해서 데이터마이닝을 공부한다'))\n",
    "\n",
    "print('\\nflatten=False\\nsent =데이터분석을 위해서 데이터마이닝을 공부한다')\n",
    "print(tokenizer.tokenize('데이터분석을 위해서 데이터마이닝을 공부한다', flatten=False))\n",
    "\n",
    "scores = {'데이':0.5, '데이터':0.5, '데이터마이닝':0.5, '공부':0.5, '공부중':0.45}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2-3] Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    한 어절에서 subword들의 점수의 차이가 그 어절의 점수 최대값과 tolerance 이하로 나타난다면, 길이가 가장 긴 어절을 선택한다\n",
    "    --다시 이해해야함. \n",
    "    --어절의 점수 최대값 : 공부(0.5), tolerance : 0.1, 공부중(0.45)\n",
    "    --tolerance + 공부중(0.45) > 공부(0.5) 때문에 출력되는게 아닐까?\n",
    "    --tolerance 0.04로 설정해서 검증 : 맞네!\n",
    "    검증 결과 : ('어절의 점수 최대값') < ('어절에서 두번째로 높은 값' + 'tolerance') 라면, 두번째로 높은 값이 출력됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tolerance=0.0\n",
      "sent = 데이터마이닝을 공부중이다\n",
      "['데이터마이닝', '을', '공부', '중이다']\n",
      "tolerance=0.1\n",
      "sent = 데이터마이닝을 공부중이다\n",
      "['데이터마이닝', '을', '공부중', '이다']\n",
      "tolerance=0.04\n",
      "sent = 데이터마이닝을 공부중이다\n",
      "['데이터마이닝', '을', '공부', '중이다']\n"
     ]
    }
   ],
   "source": [
    "print('tolerance=0.0\\nsent = 데이터마이닝을 공부중이다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부중이다'))\n",
    "\n",
    "print('tolerance=0.1\\nsent = 데이터마이닝을 공부중이다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부중이다', tolerance=0.1))\n",
    "\n",
    "print('tolerance=0.04\\nsent = 데이터마이닝을 공부중이다')\n",
    "print(tokenizer.tokenize('데이터마이닝을 공부중이다', tolerance=0.04))\n",
    "\n",
    "scores = {'데이':0.5, '데이터':0.5, '데이터마이닝':0.5, '공부':0.5, '공부중':0.45}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RegexTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    RegexTokenizer는 regular expression을 이용해 언어가 달라지는순간들을 띄어쓴다. \n",
    "    영어의 경우에는 움라우트가 들어가는 경우가 있어서 알파벳 뿐만 아니라 라틴까지 포함한다. \n",
    "    또한, 숫자, 한글, 외래어(영어)등도 구분해준다.\n",
    "    \n",
    "    \n",
    "    re.colpile('[가=힣]+') : 초/중/종성이 완전한 한국어의 시작부터 끝까지라는 의미\n",
    "    re.compile('[ㄱ-ㅎ]+') : ㄱ부터 ㅎ까지 자음의 범위를 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    한국어에서 자음/모음이 단어 중간에 단어의 경계를 구분해주는 역할을 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 이렇게연속된문장은잘리지않습니다만\n",
      "->['이렇게연속된문장은잘리지않습니다만']\n",
      "\n",
      " 숫자123이영어abc에섞여있으면ㅋㅋ잘리겠죠\n",
      "->['숫자', '123', '이영어', 'abc', '에섞여있으면', 'ㅋㅋ', '잘리겠죠']\n",
      "\n",
      " 띄어쓰기가 포함되어 있으면 이정보는10점!꼭띄워야죠\n",
      "->['띄어쓰기가', '포함되어', '있으면', '이정보는', '10', '점', '!', '꼭띄워야죠']\n",
      "\n",
      " 아이고ㅋㅋ진짜?\n",
      "->['아이고', 'ㅋㅋ', '진짜', '?']\n",
      "\n",
      " 아이고ㅋㅋㅜㅜ진짜?\n",
      "->['아이고', 'ㅋㅋ', 'ㅜㅜ', '진짜', '?']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "\n",
    "sents =[\n",
    "    '이렇게연속된문장은잘리지않습니다만', \n",
    "    '숫자123이영어abc에섞여있으면ㅋㅋ잘리겠죠',\n",
    "    '띄어쓰기가 포함되어 있으면 이정보는10점!꼭띄워야죠',\n",
    "    '아이고ㅋㅋ진짜?',\n",
    "    '아이고ㅋㅋㅜㅜ진짜?'\n",
    "]\n",
    "\n",
    "for sent in sents:\n",
    "    print(' %s\\n->%s\\n' % (sent, tokenizer.tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten = False 띄어쓰기 기준으로 토근을 나눠서 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['띄어쓰기가'], ['포함되어'], ['있으면'], ['이정보는', '10', '점', '!', '꼭띄워야죠']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('띄어쓰기가 포함되어 있으면 이정보는10점!꼭띄워야죠', flatten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
